{
    "data": {
        "attributes": {
            "deffile": "#\n# Ubuntu ML container\n# * TensorFlow 1.4\n# * Uber Horovod: https://eng.uber.com/horovod/ (w/ MPI)\n#\n\nBootStrap: docker\nFrom: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04\nIncludeCmd: false\n\n%labels\n\nMaintainer G. Perdue\nDate: 2017-12-05\ncuda_driver 375.26\n\n%help\n\nA portable Ubuntu 16.04 environment with TensorFlow 1.4 and Horovod\n\n%environment\n    # set these environment variables\n    export NV_DRIVER_VERSION=375.26\n    export NV_DRIVER_ROOT=/usr/local/NVIDIA-Linux-x86_64\n    export CUDA_ROOT=/usr/local/cuda\n    export CUDA_HOME=/usr/local/cuda\n    export PATH=$PATH:$NV_DRIVER_ROOT:$CUDA_ROOT/bin\n    export LD_LIBRARY_PATH=$NV_DRIVER_ROOT:$CUDA_ROOT/lib64\n\n%runscript\n    # Check the current environment\n    chk_nvidia_uvm=$(grep nvidia_uvm /proc/modules)\n    if [ -z \"$chk_nvidia_uvm\" ]; then\n        echo \"Problem detected on the host: the Linux kernel module nvidia_uvm is not loaded\"\n    fi\n    exec /bin/bash\n\n%post\n    # Runs within the container during Bootstrap\n\n    # make filesystem mount points\n    mkdir /data\n\n    # Set up some required environment defaults\n    export LC_ALL=C\n    export CUDA_HOME=/usr/local/cuda\n    export CUDA_ROOT=/usr/local/cuda\n    export PATH=/bin:/sbin:/usr/bin:/usr/sbin:${CUDA_HOME}/bin:/usr/local/NVIDIA-Linux-x86_64:$PATH\n    export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${CUDA_HOME}/extras/CUPTI/lib64:/usr/local/NVIDIA-Linux-x86_64\n\n    # Install the necessary packages (from repo)\n    apt-get update && apt-get install -y --no-install-recommends \\\n    \tautoconf \\\n\tautomake \\\n        bc \\\n\tcmake \\\n        curl \\\n\tg++ \\\n\tgfortran \\\n        git \\\n\tlibatlas-base-dev \\\n\tlibatlas3-base \\\n\tlibblas-dev \\\n\tlibboost-all-dev \\\n\tlibcupti-dev \\\n\tlibcurl4-openssl-dev \\\n\tlibffi-dev \\\n        libfreetype6-dev \\\n\tlibgflags-dev \\\n\tlibgoogle-glog-dev \\\n        libgraphviz-dev \\\n\tlibhdf5-serial-dev \\\n\tlibibverbs-dev \\\n\tlibjpeg-dev \\\n\tlibleveldb-dev \\\n\tliblcms2-dev \\\n\tliblapack-dev \\\n\tliblapacke-dev \\\n\tliblmdb-dev \\\n        libopenblas-dev \\\n\tlibopenmpi-dev \\\n\tlibopencv-dev \\\n\tlibprotobuf-dev \\\n        libpng-dev \\\n\tlibsnappy-dev \\\n\tlibssl-dev \\\n\tlibxml2-dev \\\n\tlibtiff5-dev \\\n\tlibwebp-dev \\\n        libzmq3-dev \\\n        pkg-config \\\n\tprotobuf-compiler \\\n        python-dev \\\n        python-pip \\\n        python-pydot \\\n\tpython-setuptools \\\n\tpython-tk \\\n        rsync \\\n        software-properties-common \\\n\ttime \\\n        unzip \\\n\tvim \\\n\tzip \\\n\twget \\\n        zlib1g-dev\n    apt-get clean && \\\n    apt-get autoremove && \\\n    rm -rf /var/lib/apt/lists/*\n\n    ##### INSTALL NVIDIA DRIVERS #####\n    \n    # get NVIDIA drivers\n    cd /opt/\n    wget http://us.download.nvidia.com/XFree86/Linux-x86_64/375.26/NVIDIA-Linux-x86_64-375.26.run\n    # extract .run file to /usr/local/\n    sh NVIDIA-Linux-x86_64-375.26.run -x\n    mv NVIDIA-Linux-x86_64-375.26 /usr/local/NVIDIA-Linux-x86_64\n    rm NVIDIA-Linux-x86_64-375.26.run\n    \n    # create symbolic links\n    cd /usr/local/NVIDIA-Linux-x86_64\n    for n in *.375.26; do\n        ln -v -s $n ${n%.375.26}\n    done\n    ln -v -s libnvidia-ml.so.375.26 libnvidia-ml.so.1\n    ln -v -s libcuda.so.375.26 libcuda.so.1\n\n    ##### INSTALL NCLL #####\n\n    cd /opt/\n    git clone https://github.com/NVIDIA/nccl.git\n    cd nccl/\n    make\n    make install\n    cd ../\n    rm -rf nccl\n\n    # libgpuarray with python binding\n    git clone https://github.com/Theano/libgpuarray.git\n    sdir=`pwd`\n    cd libgpuarray && \\\n    mkdir Build && \\\n    cd Build && \\\n    cmake .. -DCMAKE_BUILD_TYPE=Release && \\\n    make && \\\n    make install && \\\n    cd .. && \\\n    python2.7 setup.py install && \\\n    cd $sdir && \\\n    rm -rf libgpuarray\n    /sbin/ldconfig\n\n    # Update to the latest pip (newer than repo)\n    pip install --no-cache-dir --upgrade pip\n    pip install -U setuptools\n    # Install other needed packages\n    pip install --no-cache-dir --upgrade \\\n        chardet \\\n    \tCheetah \\\n    \tCython \\\n        deepdish \\\n        future \\\n\th5py \\\n\tipykernel \\\n\tjupyter \\\n\tleveldb \\\n\tlmdb \\\n\tMako \\\n        matplotlib \\\n\tndg-httpsclient \\\n\tnose \\\n\tnumpy \\\n\tpandas \\\n\tpath.py \\\n\tPillow \\\n\tpyasn1 \\\n\tpygments \\\n\tpyopenssl \\\n\tpython-dateutil \\\n\tpython-gflags \\\n\tpyyaml \\\n\trequests \\\n        scipy \\\n\tscikit-image \\\n        scikit-learn \\\n\tsix \\\n\tsqlalchemy \\\n\tsympy \\\n\turllib3 \\\n\tvirtualenv \\\n\twheel \\\n\tzmq\n\n    # pyCUDA\n    ( cd /tmp && \\\n      wget https://pypi.python.org/packages/b3/30/9e1c0a4c10e90b4c59ca7aa3c518e96f37aabcac73ffe6b5d9658f6ef843/pycuda-2017.1.1.tar.gz#md5=9e509f53a23e062b31049eb8220b2e3d && \\\n      tar xf pycuda-2017.1.1.tar.gz && \\\n      cd pycuda-2017.1.1 && \\\n      python configure.py --cuda-root=${CUDA_HOME} --cudadrv-lib-dir=${NV_DRIVER_ROOT} && \\\n      make install \\\n    )\n\n    # TensorFlow package versions as listed here:\n    #   https://www.tensorflow.org/get_started/os_setup#test_the_tensorflow_installation\n    #\n    # Ubuntu/Linux 64-bit, GPU enabled, Python 2.7 (tf1.3 requires CUDA toolkit 8.0 and CuDNN v6)\n    # https://www.tensorflow.org/install/install_linux#the_url_of_the_tensorflow_python_package\n    export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp27-none-linux_x86_64.whl\n    pip install --no-cache-dir --ignore-installed --upgrade $TF_BINARY_URL\n\n    # Horovod\n    pip install --no-cache-dir --upgrade horovod\n\n    /sbin/ldconfig\n\n%test\n    # Sanity check that the container is operating\n    #\n    # nosetests /usr/local/caffe/source/python\n    # nosetests \n",
            "help": "\n\n\nA portable Ubuntu 16.04 environment with TensorFlow 1.4 and Horovod\n\n",
            "labels": {
                "org.label-schema.usage.singularity.version": "2.4-feature-squashbuild-secbuild.g818b648",
                "org.label-schema.usage.singularity.deffile.bootstrap": "docker",
                "maintainer": "NVIDIA CORPORATION <cudatools@nvidia.com>",
                "org.label-schema.usage.singularity.deffile": "Singularity.tf_1_4_horovod",
                "org.label-schema.usage": "/.singularity.d/runscript.help",
                "org.label-schema.schema-version": "1.0",
                "org.label-schema.usage.singularity.deffile._readme.md_recipe.caffe-titan-0.15.13_recipe.caffe-titan-0.15.13-dann_singularity.caffe-dann_singularity.caffe-dann-nocudnn_singularity.caffe-patch2_singularity.pycaffe-patch2_singularity.tf_1_4_horovod_singularity.vanilla-caffe_wilson_cluster_uber_horovod": "https://eng.uber.com/horovod/ (w/ MPI)",
                "org.label-schema.usage.singularity.deffile.includecmd": "false",
                "MAINTAINER": "G. Perdue",
                "com.nvidia.cuda.version": "8.0.61",
                "com.nvidia.build.id": "41212532",
                "com.nvidia.volumes.needed": "nvidia_driver",
                "org.label-schema.usage.singularity.deffile.from": "nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04",
                "DATE_": "2017-12-05",
                "org.label-schema.build-date": "2017-12-08T23:58:36+00:00",
                "org.label-schema.usage.singularity.runscript.help": "/.singularity.d/runscript.help",
                "com.nvidia.build.ref": "e0edb5359ecb7bd3d86f0c9bfa18c2260b741ebb",
                "com.nvidia.cudnn.version": "6.0.21",
                "org.label-schema.build-size": "4809MB",
                "CUDA_DRIVER": "375.26"
            },
            "environment": "# Custom environment shell code should follow\n\n    # set these environment variables\n    export NV_DRIVER_VERSION=375.26\n    export NV_DRIVER_ROOT=/usr/local/NVIDIA-Linux-x86_64\n    export CUDA_ROOT=/usr/local/cuda\n    export CUDA_HOME=/usr/local/cuda\n    export PATH=$PATH:$NV_DRIVER_ROOT:$CUDA_ROOT/bin\n    export LD_LIBRARY_PATH=$NV_DRIVER_ROOT:$CUDA_ROOT/lib64\n\n",
            "runscript": "#!/bin/sh \n\n    # Check the current environment\n    chk_nvidia_uvm=$(grep nvidia_uvm /proc/modules)\n    if [ -z \"$chk_nvidia_uvm\" ]; then\n        echo \"Problem detected on the host: the Linux kernel module nvidia_uvm is not loaded\"\n    fi\n    exec /bin/bash\n\n",
            "test": "#!/bin/sh\n\n    # Sanity check that the container is operating\n    #\n    # nosetests /usr/local/caffe/source/python\n    # nosetests \n"
        },
        "type": "container"
    }
}
