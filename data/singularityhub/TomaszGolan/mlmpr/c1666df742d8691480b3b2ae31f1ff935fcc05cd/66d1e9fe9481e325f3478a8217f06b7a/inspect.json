{
    "data": {
        "attributes": {
            "deffile": "#\n# Ubuntu ML container\n# * TensorFlow 1.6\n# ** Note, TF prebuilt binaries use CUDA 9.0, cuDNN 7\n# * Uber Horovod: https://eng.uber.com/horovod/ (w/ MPI)\n# Here we use the `375.26` version of the NVIDIA drivers for Wilson Cluster compat.\n\nBootStrap: docker\nFrom: nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04\nIncludeCmd: false\n\n%labels\n\nMaintainer G. Perdue\nDate: 2018-03-13\ncuda_driver 375.26\n\n%help\n\nA portable Ubuntu 16.04 environment with TensorFlow 1.6 and Horovod\n\n%environment\n    # set these environment variables\n    export NV_DRIVER_VERSION=375.26\n    export NV_DRIVER_ROOT=/usr/local/NVIDIA-Linux-x86_64\n    export CUDA_ROOT=/usr/local/cuda\n    export CUDA_HOME=/usr/local/cuda\n    export PATH=$PATH:$NV_DRIVER_ROOT:$CUDA_ROOT/bin\n    export LD_LIBRARY_PATH=$NV_DRIVER_ROOT:$CUDA_ROOT/lib64\n\n%runscript\n    # Check the current environment\n    chk_nvidia_uvm=$(grep nvidia_uvm /proc/modules)\n    if [ -z \"$chk_nvidia_uvm\" ]; then\n        echo \"Problem detected on the host: the Linux kernel module nvidia_uvm is not loaded\"\n    fi\n    exec /bin/bash\n\n%post\n    # Runs within the container during Bootstrap\n\n    # make filesystem mount points\n    mkdir /data\n\n    # Set up some required environment defaults\n    export LC_ALL=C\n    export CUDA_HOME=/usr/local/cuda\n    export CUDA_ROOT=/usr/local/cuda\n    export PATH=/bin:/sbin:/usr/bin:/usr/sbin:${CUDA_HOME}/bin:/usr/local/NVIDIA-Linux-x86_64:$PATH\n    export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${CUDA_HOME}/extras/CUPTI/lib64:/usr/local/NVIDIA-Linux-x86_64\n\n    # Install the necessary packages (from repo)\n    apt-get update && apt-get install -y --no-install-recommends \\\n    \tautoconf \\\n      automake \\\n      bc \\\n      cmake \\\n      curl \\\n      g++ \\\n      gfortran \\\n      git \\\n      libatlas-base-dev \\\n      libatlas3-base \\\n      libblas-dev \\\n      libboost-all-dev \\\n      libcupti-dev \\\n      libcurl4-openssl-dev \\\n      libffi-dev \\\n      libfreetype6-dev \\\n      libgflags-dev \\\n      libgoogle-glog-dev \\\n      libgraphviz-dev \\\n      libhdf5-serial-dev \\\n      libibverbs-dev \\\n      libjpeg-dev \\\n      libleveldb-dev \\\n      liblcms2-dev \\\n      liblapack-dev \\\n      liblapacke-dev \\\n      liblmdb-dev \\\n      libopenblas-dev \\\n      libopenmpi-dev \\\n      libopencv-dev \\\n      libprotobuf-dev \\\n      libpng-dev \\\n      libsnappy-dev \\\n      libssl-dev \\\n      libxml2-dev \\\n      libtiff5-dev \\\n      libwebp-dev \\\n      libzmq3-dev \\\n      pkg-config \\\n      protobuf-compiler \\\n      python-dev \\\n      python-pip \\\n      python-pydot \\\n      python-setuptools \\\n      python-tk \\\n      rsync \\\n      software-properties-common \\\n      time \\\n      unzip \\\n      vim \\\n      zip \\\n      wget \\\n      zlib1g-dev\n    apt-get clean && \\\n      apt-get autoremove && \\\n      rm -rf /var/lib/apt/lists/*\n\n    ##### INSTALL NVIDIA DRIVERS #####\n    \n    # get NVIDIA drivers\n    cd /opt/\n    wget http://us.download.nvidia.com/XFree86/Linux-x86_64/375.26/NVIDIA-Linux-x86_64-375.26.run\n    # extract .run file to /usr/local/\n    sh NVIDIA-Linux-x86_64-375.26.run -x\n    mv NVIDIA-Linux-x86_64-375.26 /usr/local/NVIDIA-Linux-x86_64\n    rm NVIDIA-Linux-x86_64-375.26.run\n    \n    # create symbolic links\n    cd /usr/local/NVIDIA-Linux-x86_64\n    for n in *.375.26; do\n        ln -v -s $n ${n%.375.26}\n    done\n    ln -v -s libnvidia-ml.so.375.26 libnvidia-ml.so.1\n    ln -v -s libcuda.so.375.26 libcuda.so.1\n\n    ##### INSTALL NCLL #####\n\n    cd /opt/\n    git clone https://github.com/NVIDIA/nccl.git\n    cd nccl/\n    make\n    make install\n    cd ../\n    rm -rf nccl\n\n    # libgpuarray with python binding\n    git clone https://github.com/Theano/libgpuarray.git\n    sdir=`pwd`\n    cd libgpuarray && \\\n    mkdir Build && \\\n    cd Build && \\\n    cmake .. -DCMAKE_BUILD_TYPE=Release && \\\n    make && \\\n    make install && \\\n    cd .. && \\\n    python2.7 setup.py install && \\\n    cd $sdir && \\\n    rm -rf libgpuarray\n    /sbin/ldconfig\n\n    # Update to the latest pip (newer than repo)\n    pip install --no-cache-dir --upgrade pip\n    pip install -U setuptools\n    # Install other needed packages\n    pip install --no-cache-dir --upgrade \\\n      chardet \\\n      Cheetah \\\n    \tCython \\\n      deepdish \\\n      future \\\n      h5py \\\n      ipykernel \\\n      jupyter \\\n      leveldb \\\n      lmdb \\\n      Mako \\\n      matplotlib \\\n      ndg-httpsclient \\\n      nose \\\n      numpy \\\n      pandas \\\n      path.py \\\n      Pillow \\\n      pyasn1 \\\n      pygments \\\n      pyopenssl \\\n      python-dateutil \\\n      python-gflags \\\n      pyyaml \\\n      requests \\\n      scipy \\\n      scikit-image \\\n      scikit-learn \\\n      six \\\n      sqlalchemy \\\n      sympy \\\n      urllib3 \\\n      virtualenv \\\n      wheel \\\n      zmq\n\n    # pyCUDA\n    ( cd /tmp && \\\n      wget https://pypi.python.org/packages/b3/30/9e1c0a4c10e90b4c59ca7aa3c518e96f37aabcac73ffe6b5d9658f6ef843/pycuda-2017.1.1.tar.gz#md5=9e509f53a23e062b31049eb8220b2e3d && \\\n      tar xf pycuda-2017.1.1.tar.gz && \\\n      cd pycuda-2017.1.1 && \\\n      python configure.py --cuda-root=${CUDA_HOME} --cudadrv-lib-dir=${NV_DRIVER_ROOT} && \\\n      make install \\\n    )\n\n    # TensorFlow package versions as listed here:\n    #   https://www.tensorflow.org/get_started/os_setup#test_the_tensorflow_installation\n    #\n    # Ubuntu/Linux 64-bit, GPU enabled, Python 2.7 (tf1.6 requires CUDA toolkit 9.0 and CuDNN v7)\n    # https://www.tensorflow.org/install/install_linux#the_url_of_the_tensorflow_python_package\n    export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.6.0-cp27-none-linux_x86_64.whl\n    pip install --no-cache-dir --ignore-installed --upgrade $TF_BINARY_URL\n\n    # Horovod\n    pip install --no-cache-dir --upgrade horovod\n\n    /sbin/ldconfig\n\n%test\n    # Sanity check that the container is operating\n    #\n    # nosetests /usr/local/caffe/source/python\n    # nosetests \n",
            "help": "\n\n\nA portable Ubuntu 16.04 environment with TensorFlow 1.6 and Horovod\n\n",
            "labels": {
                "org.label-schema.usage.singularity.deffile.bootstrap": "docker",
                "maintainer": "NVIDIA CORPORATION <cudatools@nvidia.com>",
                "org.label-schema.usage.singularity.deffile": "Singularity.tf_1_6_horovod",
                "org.label-schema.usage": "/.singularity.d/runscript.help",
                "org.label-schema.schema-version": "1.0",
                "com.nvidia.cudnn.version": "7.1.1.5",
                "org.label-schema.usage.singularity.deffile.includecmd": "false",
                "org.label-schema.build-size": "5919MB",
                "org.label-schema.usage.singularity.runscript.help": "/.singularity.d/runscript.help",
                "com.nvidia.volumes.needed": "nvidia_driver",
                "org.label-schema.usage.singularity.deffile.from": "nvidia/cuda:9.0-cudnn7-devel-ubuntu16.04",
                "DATE_": "2018-03-13",
                "org.label-schema.build-date": "Tue,_20_Mar_2018_23:20:34_+0000",
                "com.nvidia.build.ref": "eb0b0e533c1a971e5204356d1201ca8b8f2d3f76",
                "com.nvidia.cuda.version": "9.0.176",
                "MAINTAINER": "G. Perdue",
                "org.label-schema.usage.singularity.version": "2.4.1-feature-squashbuild-secbuild-2.4.1.gaa08d4d",
                "com.nvidia.build.id": "56664962",
                "CUDA_DRIVER": "375.26"
            },
            "environment": "# Custom environment shell code should follow\n\n    # set these environment variables\n    export NV_DRIVER_VERSION=375.26\n    export NV_DRIVER_ROOT=/usr/local/NVIDIA-Linux-x86_64\n    export CUDA_ROOT=/usr/local/cuda\n    export CUDA_HOME=/usr/local/cuda\n    export PATH=$PATH:$NV_DRIVER_ROOT:$CUDA_ROOT/bin\n    export LD_LIBRARY_PATH=$NV_DRIVER_ROOT:$CUDA_ROOT/lib64\n\n",
            "runscript": "#!/bin/sh \n\n    # Check the current environment\n    chk_nvidia_uvm=$(grep nvidia_uvm /proc/modules)\n    if [ -z \"$chk_nvidia_uvm\" ]; then\n        echo \"Problem detected on the host: the Linux kernel module nvidia_uvm is not loaded\"\n    fi\n    exec /bin/bash\n\n",
            "test": "#!/bin/sh\n\n    # Sanity check that the container is operating\n    #\n    # nosetests /usr/local/caffe/source/python\n    # nosetests \n"
        },
        "type": "container"
    }
}
